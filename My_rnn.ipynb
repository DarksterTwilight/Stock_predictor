{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "My_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarksterTwilight/Stock_predictor/blob/main/My_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpAuMVCwfWs8"
      },
      "source": [
        "# Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxJfRe4bfYVA"
      },
      "source": [
        "## Part 1 - Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir9zwETrfbrp"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT1f24vHffuf"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ47JAxrgmaL"
      },
      "source": [
        "### Importing the training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bUCoLUqek0s"
      },
      "source": [
        "dataset_train = pd.read_csv('Google_Stock_Price_Train.csv')\n",
        "training_set = dataset_train.iloc[:, 1:2].values # 1:2 writter as python ignors the upper bound\n",
        "# values -- it is done so to make a numpy array rather than a simple vector"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XCtNGXVuwK5",
        "outputId": "aa920be5-4b93-47c3-c21a-9c6b60053889"
      },
      "source": [
        "print(training_set)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[325.25]\n",
            " [331.27]\n",
            " [329.83]\n",
            " ...\n",
            " [793.7 ]\n",
            " [783.33]\n",
            " [782.75]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT8_2UJegtG5"
      },
      "source": [
        "### Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaVAwfzcv31k"
      },
      "source": [
        "# when ever there is sigmoidal funtion as your output layer Normalization \n",
        "# is prefered over standardization\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc = MinMaxScaler(feature_range = (0, 1))\n",
        "training_set_scaled = sc.fit_transform(training_set)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lg_viKSQXB3A",
        "outputId": "9e3e8fd5-2f9a-46aa-81ea-ed9cfec40b96"
      },
      "source": [
        "print(training_set_scaled)\n",
        "print(len(training_set_scaled))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.08581368]\n",
            " [0.09701243]\n",
            " [0.09433366]\n",
            " ...\n",
            " [0.95725128]\n",
            " [0.93796041]\n",
            " [0.93688146]]\n",
            "1258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyYgYocqhNUg"
      },
      "source": [
        "### Creating a data structure with 60 timesteps and 1 output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UDJ3E7vXHX5"
      },
      "source": [
        "# 60 time step---- at each time step RNN look for data at that time step\n",
        "# and previous 60 time step to predict it's output......\n",
        "# this(60) is a hit and try no.\n",
        "X_train = []\n",
        "y_train = []\n",
        "for i in range(60, 1258):\n",
        "  X_train.append(training_set_scaled[i-60:i, 0]) \n",
        "  y_train.append(training_set_scaled[i , 0])\n",
        "  # upper bound is excluded\n",
        "\n",
        "X_train, y_train = np.array(X_train), np.array(y_train)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JV4SFVBccuH9",
        "outputId": "f9f6eec2-890a-4346-a73f-2daa4feec5db"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(len(X_train))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1198, 60)\n",
            "1198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVGLGyIeczVS",
        "outputId": "8dc2a02f-ce40-476a-920e-250a848db060"
      },
      "source": [
        "print(y_train)\n",
        "print(len(y_train))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.08627874 0.08471612 0.07454052 ... 0.95725128 0.93796041 0.93688146]\n",
            "1198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8yaN7Zvi95l"
      },
      "source": [
        "### Reshaping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMhisMjnpto-"
      },
      "source": [
        "# reshaping X_train so that new features can be added in future\n",
        "# currently X_train is an 2-d array\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRRSOJeVjEWV"
      },
      "source": [
        "## Part 2 - Building and Training the RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4XV88JMjHXG"
      },
      "source": [
        "### Importing the Keras libraries and packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tc29-NVpuMU"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEIE-1s9jNzC"
      },
      "source": [
        "### Initialising the RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLkFR-1kpu6x"
      },
      "source": [
        "regressor = Sequential()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62eg1OPGjT8z"
      },
      "source": [
        "### Adding the first LSTM layer and some Dropout regularisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuFZdpBUpvZ1"
      },
      "source": [
        "# regularilation for avoid overfitting\n",
        "regressor.add(LSTM(units= 50, return_sequences= True, input_shape = (X_train.shape[1], 1)))\n",
        "# units = no. of nurons in this hidden layer\n",
        "# return seq. true only if u anna add another lstm  to next layer\n",
        "\n",
        "# Adding Dropout regularisation\n",
        "regressor.add(Dropout(0.20))\n",
        "# 20% dropout 20% of nurons will be ignored during each iteration"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XBIYLyOjlMx"
      },
      "source": [
        "### Adding a second LSTM layer and some Dropout regularisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnuVr40ppv1h"
      },
      "source": [
        "regressor.add(LSTM(units= 50 , return_sequences= True))\n",
        "# As input shape is being mentioned in first layer there is no need to mention it in 2nd layer\n",
        "# or anyother mooving forward\n",
        "regressor.add(Dropout(0.20))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey3fHVnGj1cu"
      },
      "source": [
        "### Adding a third LSTM layer and some Dropout regularisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quw7GrGZpwa6"
      },
      "source": [
        "regressor.add(LSTM(units= 50 , return_sequences= True))\n",
        "regressor.add(Dropout(0.20))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYTrtfTmj933"
      },
      "source": [
        "### Adding a fourth LSTM layer and some Dropout regularisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqfG_f6GpxZv"
      },
      "source": [
        "regressor.add(LSTM(units= 50 , return_sequences= False))\n",
        "# since next is the o/p layer\n",
        "regressor.add(Dropout(0.20))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ABI6rOIkHhk"
      },
      "source": [
        "### Adding the output layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iexW50uxpyDi"
      },
      "source": [
        "regressor.add(Dense(units=1))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLx4K7uUkPSh"
      },
      "source": [
        "### Compiling the RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6hX87m1pzIV"
      },
      "source": [
        "regressor.compile(optimizer= 'adam',loss= 'mean_squared_error')\n",
        "# as per keras documentation for RNN\n",
        "# RMSprop is recommended for RNN\n",
        "# But we will use use adam optimizer (More trusted by me personally)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mPhwKGkkebi"
      },
      "source": [
        "### Fitting the RNN to the Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bqdnoS5pzp1",
        "outputId": "88373166-ee43-4965-e947-6ad5ff6cca1f"
      },
      "source": [
        "regressor.fit(X_train,y_train, epochs= 170, batch_size= 32)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/170\n",
            "38/38 [==============================] - 15s 153ms/step - loss: 0.0384\n",
            "Epoch 2/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0063\n",
            "Epoch 3/170\n",
            "38/38 [==============================] - 6s 151ms/step - loss: 0.0063\n",
            "Epoch 4/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0061\n",
            "Epoch 5/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0053\n",
            "Epoch 6/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0050\n",
            "Epoch 7/170\n",
            "38/38 [==============================] - 6s 151ms/step - loss: 0.0052\n",
            "Epoch 8/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0044\n",
            "Epoch 9/170\n",
            "38/38 [==============================] - 6s 152ms/step - loss: 0.0045\n",
            "Epoch 10/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0051\n",
            "Epoch 11/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0044\n",
            "Epoch 12/170\n",
            "38/38 [==============================] - 6s 151ms/step - loss: 0.0041\n",
            "Epoch 13/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0043\n",
            "Epoch 14/170\n",
            "38/38 [==============================] - 6s 152ms/step - loss: 0.0046\n",
            "Epoch 15/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0049\n",
            "Epoch 16/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0039\n",
            "Epoch 17/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0037\n",
            "Epoch 18/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0042\n",
            "Epoch 19/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0035\n",
            "Epoch 20/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0038\n",
            "Epoch 21/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0036\n",
            "Epoch 22/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0038\n",
            "Epoch 23/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0031\n",
            "Epoch 24/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0037\n",
            "Epoch 25/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0032\n",
            "Epoch 26/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0034\n",
            "Epoch 27/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0033\n",
            "Epoch 28/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0036\n",
            "Epoch 29/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0030\n",
            "Epoch 30/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0034\n",
            "Epoch 31/170\n",
            "38/38 [==============================] - 6s 156ms/step - loss: 0.0031\n",
            "Epoch 32/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0032\n",
            "Epoch 33/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0031\n",
            "Epoch 34/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0029\n",
            "Epoch 35/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0029\n",
            "Epoch 36/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0030\n",
            "Epoch 37/170\n",
            "38/38 [==============================] - 6s 152ms/step - loss: 0.0027\n",
            "Epoch 38/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0029\n",
            "Epoch 39/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0027\n",
            "Epoch 40/170\n",
            "38/38 [==============================] - 6s 152ms/step - loss: 0.0026\n",
            "Epoch 41/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0027\n",
            "Epoch 42/170\n",
            "38/38 [==============================] - 6s 156ms/step - loss: 0.0027\n",
            "Epoch 43/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0027\n",
            "Epoch 44/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0025\n",
            "Epoch 45/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0027\n",
            "Epoch 46/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0025\n",
            "Epoch 47/170\n",
            "38/38 [==============================] - 6s 156ms/step - loss: 0.0023\n",
            "Epoch 48/170\n",
            "38/38 [==============================] - 6s 152ms/step - loss: 0.0025\n",
            "Epoch 49/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0026\n",
            "Epoch 50/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0028\n",
            "Epoch 51/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0025\n",
            "Epoch 52/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0027\n",
            "Epoch 53/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0023\n",
            "Epoch 54/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0024\n",
            "Epoch 55/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0025\n",
            "Epoch 56/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0023\n",
            "Epoch 57/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0023\n",
            "Epoch 58/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0024\n",
            "Epoch 59/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0023\n",
            "Epoch 60/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0022\n",
            "Epoch 61/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0021\n",
            "Epoch 62/170\n",
            "38/38 [==============================] - 6s 156ms/step - loss: 0.0024\n",
            "Epoch 63/170\n",
            "38/38 [==============================] - 6s 156ms/step - loss: 0.0019\n",
            "Epoch 64/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0020\n",
            "Epoch 65/170\n",
            "38/38 [==============================] - 6s 157ms/step - loss: 0.0019\n",
            "Epoch 66/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0021\n",
            "Epoch 67/170\n",
            "38/38 [==============================] - 6s 156ms/step - loss: 0.0020\n",
            "Epoch 68/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0018\n",
            "Epoch 69/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0019\n",
            "Epoch 70/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0018\n",
            "Epoch 71/170\n",
            "38/38 [==============================] - 6s 156ms/step - loss: 0.0019\n",
            "Epoch 72/170\n",
            "38/38 [==============================] - 6s 156ms/step - loss: 0.0019\n",
            "Epoch 73/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0019\n",
            "Epoch 74/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0020\n",
            "Epoch 75/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0018\n",
            "Epoch 76/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0017\n",
            "Epoch 77/170\n",
            "38/38 [==============================] - 6s 156ms/step - loss: 0.0017\n",
            "Epoch 78/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0019\n",
            "Epoch 79/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0017\n",
            "Epoch 80/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0020\n",
            "Epoch 81/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0017\n",
            "Epoch 82/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0017\n",
            "Epoch 83/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0017\n",
            "Epoch 84/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0016\n",
            "Epoch 85/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0016\n",
            "Epoch 86/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0016\n",
            "Epoch 87/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0015\n",
            "Epoch 88/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0016\n",
            "Epoch 89/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0016\n",
            "Epoch 90/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0017\n",
            "Epoch 91/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0017\n",
            "Epoch 92/170\n",
            "38/38 [==============================] - 6s 156ms/step - loss: 0.0017\n",
            "Epoch 93/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0018\n",
            "Epoch 94/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0016\n",
            "Epoch 95/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0014\n",
            "Epoch 96/170\n",
            "38/38 [==============================] - 6s 152ms/step - loss: 0.0015\n",
            "Epoch 97/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0014\n",
            "Epoch 98/170\n",
            "38/38 [==============================] - 6s 152ms/step - loss: 0.0015\n",
            "Epoch 99/170\n",
            "38/38 [==============================] - 6s 152ms/step - loss: 0.0015\n",
            "Epoch 100/170\n",
            "38/38 [==============================] - 6s 152ms/step - loss: 0.0015\n",
            "Epoch 101/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0015\n",
            "Epoch 102/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0014\n",
            "Epoch 103/170\n",
            "38/38 [==============================] - 6s 156ms/step - loss: 0.0016\n",
            "Epoch 104/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0014\n",
            "Epoch 105/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0014\n",
            "Epoch 106/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0013\n",
            "Epoch 107/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0013\n",
            "Epoch 108/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0013\n",
            "Epoch 109/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0013\n",
            "Epoch 110/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0015\n",
            "Epoch 111/170\n",
            "38/38 [==============================] - 6s 152ms/step - loss: 0.0014\n",
            "Epoch 112/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0013\n",
            "Epoch 113/170\n",
            "38/38 [==============================] - 6s 156ms/step - loss: 0.0012\n",
            "Epoch 114/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0012\n",
            "Epoch 115/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0012\n",
            "Epoch 116/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0012\n",
            "Epoch 117/170\n",
            "38/38 [==============================] - 6s 156ms/step - loss: 0.0013\n",
            "Epoch 118/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0013\n",
            "Epoch 119/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0016\n",
            "Epoch 120/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0012\n",
            "Epoch 121/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0012\n",
            "Epoch 122/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0012\n",
            "Epoch 123/170\n",
            "38/38 [==============================] - 6s 156ms/step - loss: 0.0012\n",
            "Epoch 124/170\n",
            "38/38 [==============================] - 6s 152ms/step - loss: 0.0013\n",
            "Epoch 125/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0011\n",
            "Epoch 126/170\n",
            "38/38 [==============================] - 6s 152ms/step - loss: 0.0013\n",
            "Epoch 127/170\n",
            "38/38 [==============================] - 6s 152ms/step - loss: 0.0011\n",
            "Epoch 128/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0011\n",
            "Epoch 129/170\n",
            "38/38 [==============================] - 6s 156ms/step - loss: 0.0011\n",
            "Epoch 130/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0012\n",
            "Epoch 131/170\n",
            "38/38 [==============================] - 6s 151ms/step - loss: 0.0011\n",
            "Epoch 132/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0011\n",
            "Epoch 133/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0010\n",
            "Epoch 134/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0011\n",
            "Epoch 135/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0011\n",
            "Epoch 136/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0010\n",
            "Epoch 137/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0011\n",
            "Epoch 138/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0011\n",
            "Epoch 139/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0013\n",
            "Epoch 140/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0012\n",
            "Epoch 141/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0011\n",
            "Epoch 142/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0011\n",
            "Epoch 143/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0011\n",
            "Epoch 144/170\n",
            "38/38 [==============================] - 6s 152ms/step - loss: 0.0012\n",
            "Epoch 145/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0011\n",
            "Epoch 146/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 9.8851e-04\n",
            "Epoch 147/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0011\n",
            "Epoch 148/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0011\n",
            "Epoch 149/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0011\n",
            "Epoch 150/170\n",
            "38/38 [==============================] - 6s 152ms/step - loss: 0.0013\n",
            "Epoch 151/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0012\n",
            "Epoch 152/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0010\n",
            "Epoch 153/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 9.8820e-04\n",
            "Epoch 154/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 9.8678e-04\n",
            "Epoch 155/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0010\n",
            "Epoch 156/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0013\n",
            "Epoch 157/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0011\n",
            "Epoch 158/170\n",
            "38/38 [==============================] - 6s 156ms/step - loss: 0.0010\n",
            "Epoch 159/170\n",
            "38/38 [==============================] - 6s 156ms/step - loss: 9.2327e-04\n",
            "Epoch 160/170\n",
            "38/38 [==============================] - 6s 156ms/step - loss: 9.6174e-04\n",
            "Epoch 161/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 9.7561e-04\n",
            "Epoch 162/170\n",
            "38/38 [==============================] - 6s 153ms/step - loss: 0.0013\n",
            "Epoch 163/170\n",
            "38/38 [==============================] - 6s 156ms/step - loss: 0.0013\n",
            "Epoch 164/170\n",
            "38/38 [==============================] - 6s 157ms/step - loss: 0.0010\n",
            "Epoch 165/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 9.4917e-04\n",
            "Epoch 166/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0011\n",
            "Epoch 167/170\n",
            "38/38 [==============================] - 6s 156ms/step - loss: 9.0762e-04\n",
            "Epoch 168/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0010\n",
            "Epoch 169/170\n",
            "38/38 [==============================] - 6s 154ms/step - loss: 0.0010\n",
            "Epoch 170/170\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.0011\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa6d41abf90>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hRau_lIkrE8"
      },
      "source": [
        "## Part 3 - Making the predictions and visualising the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgJO6qEDksxD"
      },
      "source": [
        "### Getting the real stock price of 2017"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C1ukQtDp0gU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrvrLblxkz42"
      },
      "source": [
        "### Getting the predicted stock price of 2017"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzcalWo0p1Am"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFTNs3YHk6FQ"
      },
      "source": [
        "### Visualising the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWfqlhICp1n4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}